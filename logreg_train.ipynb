{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c88cddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from toolkit_dslr.lr_utils import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b82c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.cost_history = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def cost(self, h, y):\n",
    "        \"\"\"Cross-entropy loss\"\"\"\n",
    "        epsilon = 1e-5\n",
    "        m = len(y)\n",
    "        return - (1/m) * np.sum(y*np.log(h + epsilon) + (1-y)*np.log(1-h + epsilon))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train model using gradient descent\"\"\"\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            h = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "            dw = (1/m) * np.dot(X.T, (h - y))\n",
    "            db = (1/m) * np.sum(h - y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "            self.cost_history.append(self.cost(h, y))\n",
    "\n",
    "        return self.weights, self.bias\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.sigmoid(np.dot(X, self.weights)\n",
    "                             + self.bias)\n",
    "\n",
    "    def predict_arguments(self, X, weights, bias):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.sigmoid(np.dot(X, weights) + bias)\n",
    "\n",
    "    def sgd(self, X, y, batch_size=1):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n) \n",
    "        self.cost_history = []\n",
    "\n",
    "        for epoch in range(self.iterations):\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                h_batch = self.predict(X_batch)\n",
    "                dw = (1/batch_size) * np.dot(X_batch.T, (h_batch - y_batch))\n",
    "                db = (1/batch_size) * np.sum(h_batch - y_batch)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            h_all = self.predict(X)\n",
    "            cost = self.cost(h_all, y)\n",
    "            self.cost_history.append(cost)\n",
    "\n",
    "    def mini_batch_fit(self, X, y, batch_size=32):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "        self.cost_history = []\n",
    "\n",
    "        for epoch in range(self.iterations):\n",
    "            for i in range(0, m, batch_size):\n",
    "                # Subset data for batches\n",
    "                X_new = X[i:i+batch_size]\n",
    "                y_new = y[i:i+batch_size]\n",
    "\n",
    "                y_pred = self.predict(X_new)\n",
    "\n",
    "                # Update weights and bias\n",
    "                dw = (1/batch_size) * np.dot(X_new.T, (y_pred - y_new))\n",
    "                db = (1/batch_size) * np.sum(y_pred - y_new)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            h_all = self.predict(X)\n",
    "            cost = self.cost(h_all, y)\n",
    "            self.cost_history.append(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16122d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(file: str):\n",
    "    df = pd.read_csv(file)\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "    houses = [\"Gryffindor\", \"Slytherin\", \"Ravenclaw\", \"Hufflepuff\"]\n",
    "    X = preprocess_data(df)\n",
    "\n",
    "    X_train, X_test, y_train_global, y_test_global = train_test_split(\n",
    "        X, df[\"Hogwarts House\"].values, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    models = {}\n",
    "    for house in houses:\n",
    "        y_train = np.array([1 if i == house else 0 for i in y_train_global])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        model = LogisticRegressionScratch(learning_rate=0.1, iterations=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        plt.plot(model.cost_history)\n",
    "        plt.title(\"Cost Function Convergence\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        models[house] = (model, scaler)\n",
    "\n",
    "\n",
    "    house_preds = []\n",
    "    for idx in range(X_test.shape[0]):\n",
    "        probas = []\n",
    "        for house in houses:\n",
    "            model, scaler = models[house]\n",
    "            x = X_test[idx].reshape(1, -1)\n",
    "            proba = model.predict(x)[0]\n",
    "            probas.append(proba)\n",
    "        best_house_idx = np.argmax(probas)\n",
    "        house_preds.append(houses[best_house_idx])\n",
    "\n",
    "    accuracy = np.mean(house_preds == y_test_global)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "14e7c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(file: str):\n",
    "    df = pd.read_csv(file)\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "    houses = [\"Gryffindor\", \"Slytherin\", \"Ravenclaw\", \"Hufflepuff\"]\n",
    "    X = preprocess_data(df)\n",
    "\n",
    "    X_train, X_test, y_train_global, y_test_global = train_test_split(\n",
    "        X, df[\"Hogwarts House\"].values, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    models = {}\n",
    "    for house in houses:\n",
    "        y_train = np.array([1 if i == house else 0 for i in y_train_global])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        model = LogisticRegressionScratch(learning_rate=0.1, iterations=1000)\n",
    "\n",
    "        # Using Stochastic gradient descent with batch size of 1\n",
    "        model.sgd(X_train, y_train)\n",
    "\n",
    "        plt.plot(model.cost_history)\n",
    "        plt.title(\"Cost Function Convergence\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        models[house] = (model, scaler)\n",
    "\n",
    "\n",
    "    house_preds = []\n",
    "    for idx in range(X_test.shape[0]):\n",
    "        probas = []\n",
    "        for house in houses:\n",
    "            model, scaler = models[house]\n",
    "            x = X_test[idx].reshape(1, -1)\n",
    "            proba = model.predict(x)[0]\n",
    "            probas.append(proba)\n",
    "        best_house_idx = np.argmax(probas)\n",
    "        house_preds.append(houses[best_house_idx])\n",
    "\n",
    "    accuracy = np.mean(house_preds == y_test_global)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f6070a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(file: str):\n",
    "    df = pd.read_csv(file)\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "    houses = [\"Gryffindor\", \"Slytherin\", \"Ravenclaw\", \"Hufflepuff\"]\n",
    "    X = preprocess_data(df)\n",
    "\n",
    "    X_train, X_test, y_train_global, y_test_global = train_test_split(\n",
    "        X, df[\"Hogwarts House\"].values, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    models = {}\n",
    "    for house in houses:\n",
    "        y_train = np.array([1 if i == house else 0 for i in y_train_global])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        model = LogisticRegressionScratch(learning_rate=0.1, iterations=1000)\n",
    "        \n",
    "        # Using mini-batch gradient descent with batch size of 32\n",
    "        model.mini_batch_fit(X_train, y_train, batch_size=32)\n",
    "\n",
    "        plt.plot(model.cost_history)\n",
    "        plt.title(\"Cost Function Convergence\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        models[house] = (model, scaler)\n",
    "\n",
    "\n",
    "    house_preds = []\n",
    "    for idx in range(X_test.shape[0]):\n",
    "        probas = []\n",
    "        for house in houses:\n",
    "            model, scaler = models[house]\n",
    "            x = X_test[idx].reshape(1, -1)\n",
    "            proba = model.predict(x)[0]\n",
    "            probas.append(proba)\n",
    "        best_house_idx = np.argmax(probas)\n",
    "        house_preds.append(houses[best_house_idx])\n",
    "\n",
    "    accuracy = np.mean(house_preds == y_test_global)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dbdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression(\"datasets/dataset_train.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
